{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering of movie reviews dataset.\n",
    "\n",
    "Pre-process the data by tokenizing, removing stop words and punctuation.\n",
    "\n",
    "Convert each review from a list of words to a tf-idf weighted vector. \n",
    "\n",
    "Define the similarity of documents as the cosine similarity of the document vectors.\n",
    "\n",
    "Apply K-means on these set of vectors\n",
    "\n",
    "Attempt to interpret the strange clusters obtained o_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = nltk.corpus.movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_words = data.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1583820"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.paras())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('English'))\n",
    "valid_words = set(words.words())\n",
    "counter = FreqDist(w.lower() for w in text_words)\n",
    "for k, v in list(counter.items()):\n",
    "    if k in stop_words:\n",
    "        del counter[k]\n",
    "    if k in string.punctuation:\n",
    "        del counter[k]\n",
    "    if k not in valid_words:\n",
    "        del counter[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18308\n"
     ]
    }
   ],
   "source": [
    "print(len(counter.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = counter.most_common()[:500]\n",
    "word_to_idx = {word[0]: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_vec(words, vocab, word_to_idx, n_docs, doc_freq):\n",
    "    vec = np.zeros((len(vocab), ))\n",
    "    # term frequencies - tf\n",
    "    for word in words:\n",
    "        if word in word_to_idx:\n",
    "            vec[word_to_idx[word]] += 1\n",
    "    for i in range(vec.shape[0]):\n",
    "        # tf-idf weighting\n",
    "        if vec[i] > 0:\n",
    "            vec[i] = vec[i] * np.log(n_docs / doc_freq[vocab[i][0]])\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "doc_freq = {k: 0 for k, v in word_to_idx.items()}\n",
    "n_docs = len(data.fileids())\n",
    "for fileid in data.fileids():\n",
    "     # collect document frequencies - df\n",
    "    doc_words = set([w.lower() for w in data.words(fileid)])\n",
    "    for word in word_to_idx:\n",
    "        if word in doc_words:\n",
    "            doc_freq[word] += 1\n",
    "\n",
    "for fileid in data.fileids():\n",
    "    # collect term frequencies - tf\n",
    "    doc_words = [w.lower() for w in data.words(fileid)]\n",
    "    f = doc_to_vec(doc_words, vocab, word_to_idx, n_docs, doc_freq)\n",
    "    features.append(f)\n",
    "\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101,)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0][features[0] > 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 500)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cluster:\n",
    "    \n",
    "    def __init__(self, mean):\n",
    "        self.mean = mean\n",
    "        self.pts = []\n",
    "    \n",
    "    def update_mean(self):\n",
    "        new_mean = np.zeros_like(self.mean)\n",
    "        for pt in self.pts:\n",
    "            new_mean += pt\n",
    "        self.mean = new_mean / len(self.pts)\n",
    "    \n",
    "    def clear_pts(self):\n",
    "        self.pts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a)*np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eucl_dist(a, b):\n",
    "    return -np.linalg.norm(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 1, objective = 715.2283\n",
      "iter = 2, objective = 718.9043\n",
      "iter = 3, objective = 721.4973\n",
      "iter = 4, objective = 722.9342\n",
      "iter = 5, objective = 723.8884\n",
      "iter = 6, objective = 724.5757\n",
      "iter = 7, objective = 725.0663\n",
      "iter = 8, objective = 725.7106\n",
      "iter = 9, objective = 726.5087\n",
      "iter = 10, objective = 727.8137\n",
      "iter = 11, objective = 728.8014\n",
      "iter = 12, objective = 729.4785\n",
      "iter = 13, objective = 729.8417\n",
      "iter = 14, objective = 729.9561\n",
      "iter = 15, objective = 729.9619\n"
     ]
    }
   ],
   "source": [
    "n, d = features.shape\n",
    "k = 2\n",
    "means = features[np.random.choice(n, k)]\n",
    "clust = [Cluster(m) for m in means]\n",
    "# print(means.shape)\n",
    "n_iters = 15\n",
    "for e in range(n_iters):\n",
    "    # assign each point to nearest clusters\n",
    "    for i in range(n):\n",
    "        max_sim = -1\n",
    "        arg_max = 0\n",
    "        for j in range(k):\n",
    "            sim = cosine_sim(features[i], clust[j].mean)\n",
    "#             print(sim)\n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "                arg_max = j\n",
    "        clust[arg_max].pts.append(features[i])\n",
    "    \n",
    "    for j in range(k):\n",
    "        clust[j].update_mean()\n",
    "    \n",
    "    sim = 0\n",
    "    for j in range(k):\n",
    "        for vec in clust[j].pts:\n",
    "            sim += cosine_sim(vec, clust[j].mean)\n",
    "    print('iter = {}, objective = {:.4f}'.format(e + 1, sim))\n",
    "    \n",
    "    for j in range(k):\n",
    "        clust[j].clear_pts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_vecs = []\n",
    "for j in range(k):\n",
    "    mean = clust[j].mean\n",
    "    word_idx = np.nonzero(mean > 0.9)[0]\n",
    "    avg_vecs.append([vocab[i] for i in word_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean vector of cluster 0 - the average representation of the cluster\n",
    "avg_vecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('action', 1172),\n",
       " ('star', 761),\n",
       " ('original', 712),\n",
       " ('effects', 649),\n",
       " ('special', 574),\n",
       " ('series', 548),\n",
       " ('horror', 473),\n",
       " ('human', 432),\n",
       " ('alien', 378),\n",
       " ('summer', 334),\n",
       " ('earth', 317),\n",
       " ('computer', 273),\n",
       " ('space', 267),\n",
       " ('ship', 264),\n",
       " ('scream', 262),\n",
       " ('fiction', 258),\n",
       " ('planet', 243),\n",
       " ('smith', 236),\n",
       " ('science', 235),\n",
       " ('crew', 214),\n",
       " ('mission', 213)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean vector of cluster 1 - the average representation of the cluster\n",
    "avg_vecs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75642478])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust[0].mean[clust[0].mean > 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89347136, 0.81788881, 0.70788938, 0.71622312, 0.79392251,\n",
       "       1.04170727, 0.71005731, 0.70251227, 0.76585342, 1.59047126,\n",
       "       1.0459618 , 1.95331565, 1.46327477, 1.18413765, 1.55040296,\n",
       "       0.97317938, 3.31374067, 0.90694969, 1.59336817, 0.70155978,\n",
       "       0.79717898, 1.08250741, 1.51140766, 2.3904302 , 2.15879713,\n",
       "       1.24189822, 1.92197936, 0.84775047, 1.55108568, 1.55273209,\n",
       "       1.21857142, 1.01829405, 0.73852813])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust[1].mean[clust[1].mean > 0.7]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
