{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Func1:\n",
    "    \"\"\"Mapping from R^d -> R\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        assert x.shape == (2, 1)\n",
    "        return x[0]**2 + x[0]*x[1] + x[1]**2\n",
    "    \n",
    "    def grad(self, x, h):\n",
    "        \"\"\"Vector of first order partial derivatives of f at x\"\"\"\n",
    "        out = np.zeros_like(x)\n",
    "        for i in range(x.shape[0]):\n",
    "            x[i] += h\n",
    "            f_plus = self.__call__(x)\n",
    "            x[i] -= 2*h\n",
    "            f_minus = self.__call__(x)\n",
    "            x[i] += h\n",
    "            out[i] = (f_plus - f_minus)/(2*h)\n",
    "        return out\n",
    "    \n",
    "    def grad2(self, x, h):\n",
    "        \"\"\"Matrix of second order partial derivatives of f at x\"\"\"\n",
    "        out = np.zeros((x.shape[0], x.shape[0]))\n",
    "        grad = self.grad(x, h)\n",
    "        for i in range(x.shape[0]):\n",
    "            grad[i] += h\n",
    "            grad_plus = self.grad(grad, h)\n",
    "            grad[i] -= 2*h\n",
    "            grad_minus = self.grad(grad, h)\n",
    "            grad[i] += h\n",
    "            out[i] = np.squeeze(((grad_plus) - (grad_minus))/(2*h))\n",
    "        return out       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([91.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = Func1()\n",
    "x = np.array([5, 6], dtype = np.float)\n",
    "x = x[:, None]\n",
    "f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.],\n",
       "       [17.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.grad(x, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.00003569, 1.00044417],\n",
       "       [1.00044417, 1.99975148]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.grad2(x, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NewtonUpdate(f, x, h):\n",
    "    \"\"\"Expensive - O(d^2) f evals and O(d^3) for H^-1, but much faster convergence\"\"\"\n",
    "    delta = np.matmul(np.linalg.inv(f.grad2(x, h)), f.grad(x, h))\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.86184366]\n",
      " [9.39361726]] [278.13439052]\n",
      "[[7.82329192e-06]\n",
      " [7.45172857e-06]] [1.75029203e-10]\n",
      "[[ 6.77626358e-21]\n",
      " [-1.69406589e-21]] [3.73081703e-41]\n",
      "[[ 2.52345232e-21]\n",
      " [-2.75726871e-21]] [7.01250623e-42]\n",
      "[[0.]\n",
      " [0.]] [0.]\n",
      "[[0.]\n",
      " [0.]] [0.]\n"
     ]
    }
   ],
   "source": [
    "f = Func1()\n",
    "h = 1e-4\n",
    "a = 5\n",
    "b = 10\n",
    "iters = 5\n",
    "init = (b - a)*np.random.rand(2, 1) + a\n",
    "x = init\n",
    "print(init, f(x))\n",
    "for i in range(iters):\n",
    "    x -= NewtonUpdate(f, x, h)\n",
    "    print(x, f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientUpdate(f, x, h, lr):\n",
    "    \"\"\"Less expensive - O(d) f evals, but much slower convergence\"\"\"\n",
    "    delta = lr*f.grad(x, h)\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.04514294]\n",
      " [8.68382815]] [209.99583501]\n",
      "[[7.7974018 ]\n",
      " [8.42970016]] [197.58907876]\n",
      "[[0.27009887]\n",
      " [0.50154052]] [0.45996183]\n",
      "[[-0.02401087]\n",
      " [ 0.06070426]] [0.00280397]\n",
      "[[-0.01463181]\n",
      " [ 0.01637667]] [0.00024266]\n",
      "[[-0.00563357]\n",
      " [ 0.00571654]] [3.22113871e-05]\n",
      "[[-0.00207528]\n",
      " [ 0.00207923]] [4.31499107e-06]\n"
     ]
    }
   ],
   "source": [
    "f = Func1()\n",
    "h = 1e-4\n",
    "lr = 0.01\n",
    "a = 5\n",
    "b = 10\n",
    "iters = 600\n",
    "init = (b - a)*np.random.rand(2, 1) + a\n",
    "x = init\n",
    "print(init, f(x))\n",
    "for i in range(iters):\n",
    "    x -= GradientUpdate(f, x, h, lr)\n",
    "    if i % 100 == 0:\n",
    "        print(x, f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
