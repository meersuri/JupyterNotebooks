{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference paper (not my work) - http://www.ivanpoupyrev.com/wp-content/uploads/2017/01/siggraph_final.pdf\n",
    "Some experimental code to see how well gesture recognition works when using a simple CNN to do frame-by-fram gesture classification. This is a baseline model. Better results will be achieved when some memory is introduced - CNN + LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_channel = 0\n",
    "file_name = 'D:/SoliData/dsp/4_0_3.h5'\n",
    "gesture = h5py.File(file_name, 'r')\n",
    "list(gesture.keys())\n",
    "with h5py.File(file_name, 'r') as f:\n",
    "    # Data and label are numpy arrays\n",
    "    data = f['ch{}'.format(use_channel)][()]\n",
    "    label = f['label'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, 32, 32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = data.reshape(-1, 32, 32)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABcCAYAAAB+6068AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAACkVJREFUeJzt3V2MVPUZx/Hvs+wLLOwiCMq6sCzqoiXWgAIaTetro7GNaOprg/HCaJtqUps2DfGi8ca0N7XxyoRGjaZGbQqttrVaQZtWCxRKKQuuUlTAlReRKiyyu7A7Ty/OLI7MsDvLzJw5+z+/T7Jhzn/OzP/hl9kn/3Nmzqy5OyIiMvbVVLsAEREpDzV0EZFAqKGLiARCDV1EJBBq6CIigVBDFxEJhBq6iEggSmroZna9mb1rZtvNbFm5ihrLlElhyiWfMsmnTEpjp3phkZmNA7YB3wC6gfXAne7+dvnKG1uUSWHKJZ8yyadMSldbwmMXA9vd/X0AM3seWAKcNPx6a/DxTCxhymRrpIl+ehlkYJ27T1cmkUaaOELPsWJfK8qksNBzaaSJXg6T8YwyOUEPn37i7tNH2q+Uht4KfJiz3Q1ccuJOZnYfcB/AeBq5xK4pYcpk2+fdHGAvu9mxMzuU+kwgyqWTtQdzhvJyUSZ6rezzbt5hY+5Q6jMZssp/u3PkvUpr6FZgLO/8jbsvB5YDNNvU2L44ZmBVGwCr570EwKKNtwEw9Vvb4iphSGIy+fjF8wH496LnAfh6580ATLjug7hKyPWl/3fcmWSuWADAw0898aXxn957LwC1q/9V6RIKScxrpf8v7QD89YLfA3DhP+8EoOWmrrhKGJKYTLofugyAxTd2ArDuD18FYNYj/4irhBGV8qZoNzArZ3smsLu0csa2BibQR2/uUOozgSgXoD5nKPW5KJN8DUwgQyZ3KPWZjFYpK/T1QIeZzQE+Au4AvlOWqsqge20rAEsbrwTgyFvTAJhK5VbozUyhl8MA9WZWT8IysT9PAWDp9CsB2P9WCwBtVHaF3swUgPFJeq30nR710qWvfg+AD5YsB+CVZ6J/v/aT+5n87NqKzZ/ETHLtf+MsAG6ZcC0A35/7NwB++cI1tN++uSJzNjOFDBmSmklmQQ8AT7X9HYAbrm0CYPCRqpWU55QbursPmNkDwKvAOOBJd99atspO0d4fRodFxyYPAtD5wjwAZj5W+cOiGqvhPJ/PJt6aC3RRxUwGr7oIgB3frD9+cmzm6wMAfHr7JADaPoznULHGasDZRYJeK40r1wEwd2W0ffWKewDYdX0dAOdUsJlDMjPJNfNn0Wvj467FALz4ymEA2vsq08whymS8N9LL4URm0nZrdKrlOuZnR5J38FDKCh13fxl4uUy1BGGatYCzxd0XVruWhDmoTPIokxPUUoe7z612HWNVSQ09icb1Ru+ReF3074wYVuZJtPTxPwJw26RuLn30QQAa/hStOgeqVlVy1a2K3gQ9Z1WVC0mYidsPAZDp66tyJVIMXfovIhKI4FboZ675DICe9tMA+Pzb0cdYJ65YV7WaquG586M3tZ7jLFpI51GKlC6z5Z1qlyCjoBW6iEgggluh287oneezl8V+AYSISFVphS4iEojgVuiDnx0ceScRkQBphZ5CA1dfzMDVF1e7DBEpMzV0EZFABHfKZUjtjDMBOHRZO/DFpd4Cta9X5ZsERaTCtEIXEQlEsCv0gb37AGheE30zlS53F5HQaYUuIhKIYFfoQwb27K12CSIisdAKXUQkEGroIiKBUEMXEQlE8OfQ5Qv9NywCYOeN0Sd/zv31UQBq3txUtZok2Y7cHH39dOPvdB3HWBB8Q6+dMxuAgQ92VrmS6mt4eT0Ac/VHA6VITW8fAGCwynVIcXTKRUQkEMGv0LUyFzl1g+9ur3YJMgpaoYuIBEINXUQkEGroIiKBUEMXEQmEGrqISCDU0EVEAjHixxbNbBbwDDADyADL3f0xM5sKvAC0AzuA29z908qVmhx9foStrKefPgyjlTm0WQfH/CidrAW4wMxeQ5mkOhMYPhegw8z+i35/jmdyhB7SmEm5FLNCHwB+5O5fAS4F7jezecAyYLW7dwCrs9upYBgdXMhldh2LuIpu3uOwH2IH7zCVMwC2oExSnwkMnwvQo9+fL2cyjjrSmEm5jNjQ3X2Pu2/M3u4BuoBWYAnwdHa3p4GbKlVk0jTYBJptCgC1VkcjTfTTy35208Lsod2UScozgeFzAQ5kd0tVLsNlUkf90G6pyqRcRnUO3czagQXAOuBMd98DUdOHaBlW4DH3mdkGM9twjP7Sqk2gXv+cHj5jMlM5Sj8NNgFQJsok34m5AMcg3bmcmElNtiWlOZNSFN3QzWwSsAJ40N0PFfs4d1/u7gvdfWEdDadSY2IN+ACbWcN5zKfW6op+nDLJF3ImoFwKUSblV1RDN7M6omb+rLuvzA7vM7OW7P0twMeVKTGZMp5hM2uYQRtnWCsA9TTQ772AMlEmXzhZLkAdpDOXk2WSIQOkM5NyGLGhm5kBTwBd7v5ozl0vAXdnb98NvFj+8pLJ3XmbDUykidk29/j4dM5iD8e/DEyZkO5MYPhcgNOzm6nKZbhMjnF0aDNVmZRLMd+2eDlwF9BpZkN/CeEh4OfAb8zsHmAXcGtlSkyegxxgL7uYxGTW+msAnMsFzOa84x/RAw6iTFKdCQyfy062NWc/oqffn2wm3bxPGjMplxEburu/CdhJ7r6mvOWMDafZNK7lloL3XcwVrPLfbnH3VGWjTAobLhecbe6+MN6Kqm+4TBq9iUP+v46YSwqGrhQVEQmEGrqISCDU0EVEAqGGLiISCDV0EZFAqKGLiARCDV1EJBBq6CIigVBDFxEJhBq6iEgg1NBFRAKhhi4iEgg1dBGRQKihi4gEQg1dRCQQ5u7xTWa2H/gc+CS2SctjGqOreba7Ty9mR2WST5kUlpJclElhReUSa0MHMLMNY+1L/StdszKJ//krIY6alUv8z18JlapZp1xERAKhhi4iEohqNPTlVZizVJWuWZnE//yVEEfNyiX+56+EitQc+zl0ERGpDJ1yEREJRGwN3cyuN7N3zWy7mS2La97RMLNZZvaGmXWZ2VYz+0F2/GEz+8jMNmV/bijTfMqk8JzKJX8+ZZI/nzI5kbtX/AcYB7wHnA3UA/8B5sUx9yjrbAEuyt5uArYB84CHgR8rk8pmolyUiTIp7SeuFfpiYLu7v+/uR4HngSUxzV00d9/j7huzt3uALqC1QtMpk8KUSz5lkk+ZFBBXQ28FPszZ7qayTaFkZtYOLADWZYceMLPNZvakmU0pwxTKpDDlkk+Z5FMmBcTV0K3AWGI/XmNmk4AVwIPufgh4HDgHmA/sAX5RjmkKjKU9E1AuBacpMKZM8qU9k9gaejcwK2d7JrA7prlHxczqiIJ/1t1XArj7PncfdPcM8Cuiw71SKZPClEs+ZZJPmRQQV0NfD3SY2RwzqwfuAF6Kae6imZkBTwBd7v5oznhLzm43A1vKMJ0yKUy55FMm+ZRJAbXleJKRuPuAmT0AvEr07vST7r41jrlH6XLgLqDTzDZlxx4C7jSz+USHdDuA75Y6kTIpTLnkUyb5lElhulJURCQQulJURCQQaugiIoFQQxcRCYQauohIINTQRUQCoYYuIhIINXQRkUCooYuIBOL/3UlF461WKLIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2dd2c599710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 5\n",
    "frames = np.linspace(0, image.shape[0] - 1, n)\n",
    "frames = [int(x) for x in frames]\n",
    "for i in range(1, n + 1):\n",
    "    plt.subplot(1, n, i)\n",
    "    plt.imshow(image[frames[i - 1],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/deep-soli/config/file_half.json', 'r') as fp:\n",
    "    train_val_split = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 11\n",
    "path = 'D:/SoliData/dsp/'\n",
    "for i, name in enumerate(train_val_split['train']):\n",
    "    file_name = path + name + '.h5'\n",
    "    with h5py.File(file_name, 'r') as f:\n",
    "        # Data and label are numpy arrays\n",
    "        data = f['ch{}'.format(use_channel)][()]\n",
    "        label = f['label'][()]\n",
    "        data = data.reshape(-1, 32, 32)\n",
    "        data = data[:, None]\n",
    "        data = data.reshape(-1, 1, 32, 32)\n",
    "        label = np.eye(n_classes)[label]\n",
    "        if i == 0:\n",
    "            train_data = data\n",
    "            train_labels = label\n",
    "        else:\n",
    "            train_data = np.vstack((train_data, data))\n",
    "            train_labels = np.vstack((train_labels, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85531, 1, 32, 32) (85531, 1, 11)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85531, 11)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = train_labels.squeeze()\n",
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data\n",
    "y = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.5, test_size = 0.5)\n",
    "X_train = torch.from_numpy(X_train)\n",
    "X_val = torch.from_numpy(X_val)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_val = torch.from_numpy(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "val_dataset = torch.utils.data.TensorDataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=2,\n",
    "                                     pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     num_workers=2,\n",
    "                                     pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding = (1, 1))\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 16, 3, padding = (1, 1))\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 16, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 1.195\n",
      "[1,  2000] loss: 1.194\n",
      "[2,  1000] loss: 1.194\n",
      "[2,  2000] loss: 1.193\n",
      "[3,  1000] loss: 1.191\n",
      "[3,  2000] loss: 1.187\n",
      "[4,  1000] loss: 1.050\n",
      "[4,  2000] loss: 0.961\n",
      "[5,  1000] loss: 0.861\n",
      "[5,  2000] loss: 0.832\n",
      "[6,  1000] loss: 0.780\n",
      "[6,  2000] loss: 0.762\n",
      "[7,  1000] loss: 0.740\n",
      "[7,  2000] loss: 0.729\n",
      "[8,  1000] loss: 0.713\n",
      "[8,  2000] loss: 0.704\n",
      "[9,  1000] loss: 0.688\n",
      "[9,  2000] loss: 0.684\n",
      "[10,  1000] loss: 0.664\n",
      "[10,  2000] loss: 0.661\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device, dtype = torch.float)\n",
    "        labels = torch.argmax(labels, dim = 1)\n",
    "        labels = labels.to(device, dtype = torch.int64)\n",
    "#         print(inputs.shape, labels.shape)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "#         print(outputs.shape, type(outputs))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/SoliData/dsp/'\n",
    "for i, name in enumerate(train_val_split['eval']):\n",
    "    file_name = path + name + '.h5'\n",
    "    with h5py.File(file_name, 'r') as f:\n",
    "        # Data and label are numpy arrays\n",
    "        data = f['ch{}'.format(use_channel)][()]\n",
    "        label = f['label'][()]\n",
    "        data = data.reshape(-1, 32, 32)\n",
    "        data = data[:, None]\n",
    "        data = data.reshape(-1, 1, 32, 32)\n",
    "        label = np.eye(n_classes)[label]\n",
    "        if i == 0:\n",
    "            test_data = data\n",
    "            test_labels = label\n",
    "        else:\n",
    "            test_data = np.vstack((test_data, data))\n",
    "            test_labels = np.vstack((test_labels, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83926, 1, 32, 32) (83926, 1, 11)\n"
     ]
    }
   ],
   "source": [
    "print(test_data.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83926, 11)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels = test_labels.squeeze()\n",
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.from_numpy(test_data)\n",
    "y_test = torch.from_numpy(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     num_workers=2,\n",
    "                                     pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device, dtype = torch.float)\n",
    "        labels = torch.argmax(labels, dim = 1)\n",
    "        labels = labels.to(device, dtype = torch.int64)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 83926 test frames = 52.19\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy on {} test frames = {:.2f}'.format(\n",
    "     total, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
